{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1b----------------------------\n",
      "Q1b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-b370a20de83c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mMAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.87804\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.29303\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.23510\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.67820\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.89421\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#values from part a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0mtest_logl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprop_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_logl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_accratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Proposal Distribution Variance: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-b370a20de83c>\u001b[0m in \u001b[0;36msampling\u001b[0;34m(x_train, x_valid, x_test, y_train, y_valid, y_test, mean, sample_sizes)\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0my_star\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;31m# add to prediction summation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                     \u001b[0mout_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_star\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_variance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minner_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0mj\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-b370a20de83c>\u001b[0m in \u001b[0;36mr\u001b[0;34m(x, y, w, prior_var, proposal_var, mean)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprior_LL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_var\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mproposal_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-b370a20de83c>\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlikelihood\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-b370a20de83c>\u001b[0m in \u001b[0;36msigmoid_b\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_prod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Question 1 Part a\n",
    "\n",
    "from data_utils import load_dataset\n",
    "import numpy as np\n",
    "import math \n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from data_utils import load_dataset\n",
    "\n",
    "#ALVIN PANE \n",
    "#1004281118\n",
    "\n",
    "def likelihood(x, y):\n",
    "    likelihood = 1\n",
    "    for i in range(len(x)):\n",
    "        likelihood *= (sigmoid_b(x[i]) ** y[i]) * ((1 - sigmoid_b(x[i])) ** (1 - y[i]))\n",
    "    return likelihood\n",
    "\n",
    "def prior_LL(w, variance):\n",
    "    prior = 1\n",
    "    for i in range(len(w)):\n",
    "        prior *= 1 / math.sqrt(2 * math.pi * variance) * math.exp(-(w[i] ** 2) / (2 * variance))\n",
    "    return prior\n",
    "\n",
    "\n",
    "def proposal_like(w, proposal_var, mean):\n",
    "    proposal = 1\n",
    "    for i in range(len(w)):\n",
    "        proposal *= 1 / math.sqrt(2 * math.pi * proposal_var) * math.exp(-((mean[i] - w[i]) ** 2) / (2 * proposal_var))\n",
    "    return proposal\n",
    "\n",
    "def sigmoid(w,x):\n",
    "    z=np.dot(w.transpose(),x)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "#log likelihood\n",
    "def LL(w,estimates, actual):\n",
    "    accum = 0\n",
    "    for i in range(len(estimates)):\n",
    "        accum += actual[i]*np.log(sigmoid(w,estimates[i])) + (1-actual[i])*np.log(1 - sigmoid(w,estimates[i]))\n",
    "    return accum\n",
    "\n",
    "# Question 1a ---------------------------------------------------------------\n",
    "\n",
    "#Compute the Hessian for computing the Laplace Approximation.\n",
    "def Hess(x_t,w,variance):\n",
    "    #apply hessian function\n",
    "    h_l=np.zeros((len(x_t[0]),len(x_t[0])))\n",
    "    h_p=-1*np.identity(len(x_t[0]))/variance\n",
    "    for i in x_t:\n",
    "        h_l+=sigmoid(w,i)*(sigmoid(w,i)-1)*np.outer(i,i.transpose())\n",
    "    ret = h_l + h_p\n",
    "    return ret\n",
    "\n",
    "def marginal_LL(x_train,w,var,y_train):\n",
    "    dim=len(x_train[0])\n",
    "    det=np.linalg.det(-1*Hess(x_train,w,var))\n",
    "    return LL(w,x_train,y_train)-0.5*dim*(np.log(2*var*np.pi))-np.sum(np.square(w))/(2*var)+0.5*dim*np.log(2*np.pi)-0.5*np.log(det)\n",
    "\n",
    "# Question 1b -------------------------------------------------------------\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "def acc_ratio(y_t, y_e):\n",
    "    ar = (y_e == y_t).sum() / len(y_t)\n",
    "    return ar\n",
    "\n",
    "def matrix(x_data):\n",
    "    X = np.ones((len(x_data), len(x_data[0]) + 1))\n",
    "    X[:, 1:] = x_data\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid_b(z):\n",
    "    return np.divide(1, np.add(1, np.exp(-1*z)))\n",
    "\n",
    "def log_likelihood(x_prod, y_act):\n",
    "    log_p = np.dot(y_act.T, np.log(sigmoid_b(x_prod))) + np.dot(np.subtract(1, y_act).T, np.log(np.subtract(1, sigmoid_b(x_prod))))\n",
    "    return log_p\n",
    "\n",
    "def r(x, y, w, prior_var, proposal_var, mean):\n",
    "    return likelihood(x, y) * prior_LL(w, prior_var) / proposal_like(w, proposal_var, mean)\n",
    "\n",
    "\n",
    "def proposal(mean, variance):\n",
    "    return np.random.multivariate_normal(mean=mean, cov=np.eye(np.shape(mean)[0]) * variance)\n",
    "\n",
    "\n",
    "def sample_w(sample_size, mean, variance):\n",
    "    w = list()\n",
    "    for i in range(sample_size):\n",
    "        w.append(proposal(mean, variance))\n",
    "    return w\n",
    "\n",
    "\n",
    "def LL_2(y_pred, y):\n",
    "    log_p = np.dot(y.T, np.log(y_pred)) + np.dot(np.subtract(1, y).T, np.log(np.subtract(1, y_pred)))\n",
    "    return log_p\n",
    "\n",
    "\n",
    "# --- Question 2 ---\n",
    "\n",
    "def sampling(x_train, x_valid, x_test, y_train, y_valid, y_test, mean, sample_sizes):\n",
    "\n",
    "    #init\n",
    "    prior_var = 1 #given\n",
    "    variances = [1, 2, 2.5, 5, 10]\n",
    "    y_train = np.asarray(y_train, int)\n",
    "    y_valid = np.asarray(y_valid, int)\n",
    "    y_test = np.asarray(y_test, int)\n",
    "    X_train = matrix(x_train)\n",
    "    X_valid = matrix(x_valid)\n",
    "    X_test = matrix(x_test)\n",
    "    min_ll = 1000000000\n",
    "    \n",
    "    #set up proposal distribution\n",
    "    for sample_size in sample_sizes:\n",
    "        for proposal_variance in variances:\n",
    "\n",
    "            v_predict = np.zeros(np.shape(y_valid))\n",
    "            valid_dpred = np.zeros(np.shape(y_valid))\n",
    "\n",
    "            # sample s number of weights\n",
    "            w = sample_w(sample_size, mean, proposal_variance)\n",
    "\n",
    "            # compute predictions over the whole validation set\n",
    "            for d in range(len(X_valid)):\n",
    "\n",
    "                # inner sample over j\n",
    "                inner_sum = 0\n",
    "                i=0\n",
    "                while i<(sample_size):\n",
    "                #for j in range(sample_size):\n",
    "                    # compute and sum r(w_js)\n",
    "                    inner_sum += r(np.dot(X_train, w[i]), y_train, w[i], prior_var, proposal_variance, mean)\n",
    "                    i+=1\n",
    "                    \n",
    "                # outer sample over i\n",
    "                out_sum = 0\n",
    "                j=0\n",
    "                while j<sample_size:\n",
    "                #for i in range(sample_size):\n",
    "                    # compute sigmoid prediction of test point\n",
    "                    y_star = sigmoid_b(np.dot(X_valid[d], w[j]))\n",
    "                    # add to prediction summation \n",
    "                    out_sum += y_star*r(np.dot(X_train, w[j]), y_train, w[j], prior_var, proposal_variance, mean)/inner_sum\n",
    "                    j+=1\n",
    "                    \n",
    "                # classify\n",
    "                v_predict[d] = out_sum\n",
    "                if out_sum > 0.5:\n",
    "                    valid_dpred[d] = 1\n",
    "                elif out_sum < 0.5:\n",
    "                    valid_dpred[d] = 0\n",
    "                else:\n",
    "                    valid_dpred[d] = -1\n",
    "\n",
    "            valid_logl = -LL_2(v_predict, y_valid)\n",
    "            if valid_logl < min_ll:\n",
    "                min_ll = valid_logl\n",
    "                min_acc = acc_ratio(valid_dpred, y_valid)\n",
    "                optimal_var = proposal_variance\n",
    "                optimal_samps = sample_size\n",
    "\n",
    "    #update\n",
    "    x_train = np.vstack((x_train, x_valid))\n",
    "    X_train = matrix(x_train)\n",
    "    y_train = np.vstack((y_train, y_valid))\n",
    "    test_pred = np.zeros(np.shape(y_test))\n",
    "    test_discrete_pred = np.zeros(np.shape(y_test))\n",
    "    w = sample_w(optimal_samps, mean, optimal_var)\n",
    "\n",
    "    #compute predictions on test set\n",
    "    for d in range(len(X_test)):\n",
    "        # inner sample over j\n",
    "        r_sum = 0\n",
    "        for j in range(optimal_samps):\n",
    "            # compute and sum r(w_js)\n",
    "            r_sum += r(np.dot(X_train, w[j]), y_train, w[j], prior_var, optimal_var, mean)\n",
    "\n",
    "        # outer sample over i\n",
    "        pred_sum = 0\n",
    "        for i in range(optimal_samps):\n",
    "            # compute sigmoid prediction of test point\n",
    "            y_star = sigmoid_b(np.dot(X_test[d], w[i]))\n",
    "            # add to prediction summation \n",
    "            pred_sum += y_star*r(np.dot(X_train, w[i]), y_train, w[i], prior_var, optimal_var, mean)/r_sum\n",
    "\n",
    "        # make classification (discretized and continuous)\n",
    "        prediction = pred_sum\n",
    "        test_pred[d] = prediction\n",
    "        if prediction > 0.5:\n",
    "            test_discrete_pred[d] = 1\n",
    "        elif prediction < 0.5:\n",
    "            test_discrete_pred[d] = 0\n",
    "        else:\n",
    "            test_discrete_pred[d] = -1\n",
    "\n",
    "    test_accuracy_ratio = acc_ratio(test_discrete_pred, y_test)\n",
    "    test_log_likelihood = compute_log_likelihood(test_pred, y_test)\n",
    "\n",
    "    # sample s number of weights\n",
    "    w = sample_w(5000, mean, 1)\n",
    "\n",
    "    # inner sample over j\n",
    "    r_sum = 0\n",
    "    for j in range(5000):\n",
    "        # compute and sum r(w_js)\n",
    "        r_sum += r(np.dot(X_train, w[j]), y_train, w[j], prior_var, 2, mean)\n",
    "\n",
    "    # outer sample over i\n",
    "    posterior = list()\n",
    "    for i in range(5000):\n",
    "        # add to prediction summation\n",
    "        posterior.append(r(np.dot(X_train, w[i]), y_train, w[i], prior_var, 2, mean) / r_sum)\n",
    "\n",
    "    plot_posterior(mean, 1, posterior, w)\n",
    "\n",
    "    return -test_log_likelihood, test_accuracy_ratio, optimal_var, optimal_samps, min_ll, min_acc\n",
    "\n",
    "def plot_posterior(mean, variance, posterior, w):\n",
    "    # five components to each weight vector\n",
    "    for i in range(5):\n",
    "        weights = list()\n",
    "        # extract specific component of all weights\n",
    "        for j in range(len(w)):\n",
    "            weights.append(w[j][i])\n",
    "\n",
    "        # zip weights and posterior\n",
    "        weights, posterior = zip(*sorted(zip(weights, posterior)))\n",
    "\n",
    "        # set up gaussian\n",
    "        z = np.polyfit(weights, posterior, 1)\n",
    "        z = np.squeeze(z)\n",
    "        p = np.poly1d(z)\n",
    "\n",
    "        # plot\n",
    "        w_all = np.arange(min(weights), max(weights), 0.001)\n",
    "        q_w = scipy.stats.norm.pdf(w_all, mean[i], variance)\n",
    "        plt.figure(i)\n",
    "        plt.title(\"Posterior visualization: w(\" + str(i) + \"), var=\" + str(variance))\n",
    "        plt.xlabel(\"w[\" + str(i) + \"]\")\n",
    "        plt.ylabel(\"Probability\")\n",
    "        plt.plot(w_all, q_w, '-g', label=\"Proposal\")\n",
    "        plt.plot(weights, posterior, 'ob', label=\"Posterior\")\n",
    "        plt.plot(weights, p(weights),\"r--\")\n",
    "        plt.legend(loc='upper left')\n",
    " \n",
    "#MAIN\n",
    "\n",
    "Q1a = False   #SET TRUE TO SHOW ANSWER\n",
    "\n",
    "Q1b = True    #SET TRUE TO SHOW ANSWER\n",
    "\n",
    "xtrain, xvalid, xtest, ytrain, yvalid, ytest=load_dataset('iris')\n",
    "\n",
    "#merge training and valid datasets\n",
    "xtrain=np.hstack((np.ones((len(xtrain),1)),xtrain))\n",
    "xvalid=np.hstack((np.ones((len(xvalid),1)),xvalid))\n",
    "xtest=np.hstack((np.ones((len(xtest),1)),xtest))\n",
    "xtrain=np.vstack((xtrain,xvalid))\n",
    "ytrain=1*ytrain[:,(1,)]\n",
    "yvalid=1*yvalid[:,(1,)]\n",
    "ytrain=np.vstack((ytrain,yvalid))\n",
    "ytest=1*ytest[:,(1,)]\n",
    "init_weights=np.zeros((len(xtrain[0]),1))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    all_datas = ['mauna_loa', 'rosenbrock', 'pumadyn32nm', 'iris', 'mnist_small']    \n",
    "\n",
    "if Q1a:\n",
    "    print('Question 1a----------------------------')\n",
    "    print('For variance = 0.5,      ' + str(marginal_LL(xtrain,w,0.5,ytrain)))\n",
    "    print('For variance = 1,        ' + str(marginal_LL(xtrain,w,1,ytrain)))\n",
    "    print('For variance = 2,        ' + str(marginal_LL(xtrain,w,2,ytrain)))\n",
    "    \n",
    "if Q1b:\n",
    "    print('Question 1b----------------------------')\n",
    "    x_train, x_valid, x_test, y_train, y_valid, y_test = load_dataset('iris')\n",
    "    y_train, y_valid, y_test = y_train[:, (1,)], y_valid[:, (1,)], y_test[:, (1,)]\n",
    "\n",
    "\n",
    "    # --- Question 2 ---\n",
    "\n",
    "    MAP = [-0.87804, 0.29303, -1.23510, 0.67820, -0.89421] #values from part a)\n",
    "    samples=[10, 25, 50, 100, 200,300]\n",
    "    test_logl, test_accratio, prop_var, samplesize, valid_logl, valid_accratio = sampling(x_train, x_valid, x_test, y_train,y_valid,y_test, MAP, samples)\n",
    "                                                                                                                                                           \n",
    "    print('Proposal Distribution Variance: ' + str(prop_var))\n",
    "    print('Sample Size: ' + str(samplesize))\n",
    "    print('Validation Log-likelihood: ' + str(valid_logl))\n",
    "    print('Validation Accuracy Ratio: ' + str(valid_accratio))\n",
    "    print('Test Log-likelihood: ' + str(test_logl))\n",
    "    print('Test Accuracy Ratio: ' + str(test_accratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
